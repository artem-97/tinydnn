\documentclass[12pt, oneside, a4paper, openany]{scrarticle}
\usepackage
[
        a4paper,% other options: a3paper, a5paper, etc
        left=25mm,
        right=15mm,
        top=20mm,
        bottom=20mm,
]
{geometry}
\usepackage{setspace}
\setstretch{1.5}


%---------Russian-----------
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
%---------Russian-----------

%---------Graphics-----------
\usepackage{graphicx}
\graphicspath{{./img}}

\usepackage{tikz}
%---------Graphics-----------

%---------Maths-----------
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{mathtools}
%---------Maths-----------

%---------Title-----------
%\title{Работа}
%---------Title-----------

\begin{document}

\section{Введение}


Вообще говоря, нас будут интересовать две задачи:
\begin{itemize}
	\item отображение вектора из $\mathbb{R}^{n}$ в $\mathbb{R}$ (задача регрессии);
	\item отображение вектора из $\mathbb{R}^{n}$ в $\{0,1\}$ (задача классификации).
\end{itemize}

Целью будет являться минимизация функционала ошибки на обучающей выборке:
\begin{equation}
	\mathscr{L}(\bm{x}, y | \bm{w}) \to min,
\end{equation}
где $\bm{x}$ -- входной вектор, кодирующий объект, $\bm{x} \in \mathbb{R}^n$; $\bm{y}$ -- правильный ответ для объекта $\bm{x}$; также $\mathscr{L}$ зависит от параметров выбранной модели (весов) -- $\bm{w}$.

Решать данную оптимизационную задачу можно, например, методом градиентного спуска. Для этого нужно обновлять веса модели до сходимости (число итераций или стремления модуля разности функции ошибок с двух соседних шагов к нулю):
\begin{equation}
	\bm{w}_{new} = \bm{w}_{old} - \eta \nabla_{\bm{w}} \mathscr{L},
\end{equation}
где параметр $\eta$ называют скоростью обучения.

Таким образом, основным является умение находить градиентов выбранной модели вдоль ее весов.

\section{Датасет}
В качестве примера далее мы возьмем модельный датасет MNIST \cite{LeCun1998}, который представляет из себя просто набор 60000 рукописных цифр с их подписями:

%begin{figure}
%	\includegraphics[width=0.7\textwidth]{mnist_sample}
%\end{figure}

В коде 



\section{Нейронные сети}

Далее будем придерживаться принятых в физике и геометрии обозначений:
\begin{itemize}
	\item производные будем обозначать как:
	\begin{equation}
		\partial_i f \coloneqq \frac{\partial f(\bm{x})}{\partial x_i},
	\end{equation}
	\item столбцы будем нумеровать верхними индексами:
	\begin{equation}
		x^i = 
		\begin{bmatrix}
    		x^1\\
    		x^2\\
    		\vdots \\
    		x^n
    	\end{bmatrix},
	\end{equation}
	а строки нижними:
	\begin{equation}
		x_i = 
		\begin{bmatrix}
    		x_1 & x_2 & \hdots & x_n
    	\end{bmatrix}.
	\end{equation}
	\item подразумевам суммирование по повторяющимся индексам:
	\begin{equation}
		a_i^j x^i \coloneqq \sum_i a_i^j x^i.
	\end{equation}
\end{itemize}

Для нас нейронной сетью (DNN) будет просто нелинейное отображение:

\begin{equation}
	DNN : \mathbb{R}^n \to \mathbb{R}^h,
\end{equation}
которое как-бы преобразует вектор $\bm{x}$ в вектор из какого-то другого пространства, к которому дальше применяется loss-функция:

\begin{equation}
	\mathscr{L} : \mathbb{R}^d \to \mathbb{R},
\end{equation}

Можно это записать в виде:
\begin{equation}
	\mathscr{L} \circ DNN.
\end{equation}


Применение к выходу нейронной сети функции ошибки можно записать следующим образом:
\begin{equation}
	\mathscr{L}(\bm{\tilde{x}}, y),
\end{equation}
где $\bm{\tilde{x}}$ -- выход нейронной сети, $y$ -- правильный ответ на данном объекте. Градиент данной функции будет:
\begin{equation}
	\partial_i\mathscr{L}
\end{equation}


Один слой нейронной сети можно представить следующим образом:
\begin{equation}
	\sigma(a_i^jx^i),
\end{equation}

где $\sigma$ -- нелинейная функция активации, $a_i^j$ -- матрица коэффициентов слоя. Тогда градиент слоя будет выражаться как:
\begin{equation}
	\frac{\partial }{\partial a^i_j} \sigma(a^i_j x^j)^i
\end{equation}

\section{Функция потерь}



\newpage
\bibliography{paper}
\bibliographystyle{ieeetr}

\end{document}